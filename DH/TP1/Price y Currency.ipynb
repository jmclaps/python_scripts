{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de Prices y Currencys\n",
    "## 0. Imports, carga de datos y funciones\n",
    "Lo primero para poder ordenar el campo price y currency deberiamos considerar que las propiedades en Argentina se comercializan en dólares por usos y costumbre.\n",
    "\n",
    "Por lo que la prioridad siempre será tener el precio expresado en esa moneda y determinar una tasa de cambio a la moneda local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que devuelve un diccionario con unos valores estadisticos que quería ver\n",
    "def getQs(df, listaLugares,column):\n",
    "    dicc = {}\n",
    "    for lugar in listaLugares:\n",
    "        \n",
    "        #armo un subset por cada lugar\n",
    "        #se podría agregar un try catch acá para validar la columna\n",
    "        subset = df[df['place_with_parent_names'] == lugar].loc[:,column]\n",
    "\n",
    "        #calculo los Q y otras medidas del subset y los agrego al diccionario\n",
    "        q = subset.quantile([0.25,0.75])\n",
    "        dicc[lugar] = {'min':round(subset.min(),2),'0.25': round(q[0.25],2), 'media': round(subset.mean(),2),'0.75': round(q[0.75],2), 'max':round(subset.max(),2),'iiq': round(q[0.75]-q[0.25],2),'linf':round(-1.5*q[0.75]+2.5*q[0.25],2),'lsup':round(2.5*q[0.75]-1.5*q[0.25],2)}\n",
    "    return dicc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funciones para limpiar el findall\n",
    "\n",
    "def sacarValores(serie, cambio):\n",
    "    listReemplazo = []\n",
    "    for x in serie:\n",
    "        if len(x)==0:\n",
    "            listReemplazo.append(np.NaN)\n",
    "        else:\n",
    "            listReemplazo.append(listaConPlata(x,cambio))\n",
    "    return listReemplazo\n",
    "\n",
    "def listaConPlata(lista,cambio):\n",
    "    controlUSD = 0.00\n",
    "    controlARS = 0.00\n",
    "    for x in lista:\n",
    "        if (x[0].strip() == 'USD') | (x[0].strip() == 'U$D'):\n",
    "            #algo en USD\n",
    "            string = x[1].replace('.','').replace(',','')\n",
    "            try:\n",
    "                priceUSD = float(string)\n",
    "            except:\n",
    "                print(string, x[1])\n",
    "            controlUSD = getMax(controlUSD,priceUSD)\n",
    "        elif x[0].strip() == 'ARS':\n",
    "            #algo en ARS\n",
    "            string = x[1].replace('.','').replace(',','')\n",
    "            try:\n",
    "                priceARS = float(string)\n",
    "            except:\n",
    "                print(string, x[1])\n",
    "            controlARS = getMax(controlARS,priceARS)\n",
    "        else:\n",
    "            np.NaN\n",
    "        maxPriceUSD = getMax(controlUSD,controlARS*cambio)\n",
    "    # Siempre devuelvo el mayor porque siempre va a ser el más próximo al precio de mercado\n",
    "    return maxPriceUSD\n",
    "\n",
    "def getMax(A,B):\n",
    "    valor = 0\n",
    "    if A >= B:\n",
    "        valor = A\n",
    "    else:\n",
    "        valor = B\n",
    "    return valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se carga el csv y valido los null\n",
    "properati = pd.read_csv('./properati.csv', dtype={'operation' : 'category','property_type' : 'category','place_name' : 'category','country_name' : 'category','state_name' : 'category','currency' : 'category'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambio el nombre de la primer columna que viene sin nombre\n",
    "properati = properati.rename(columns={'Unnamed: 0':'ID'})\n",
    "properati.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total de registros\n",
    "print(f\"En total el set tiene {properati['ID'].count()} filas\")\n",
    "\n",
    "totalRegistros = properati['ID'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Homogeneizar Monedas\n",
    "Lo más lógico sería comprender la distribución en base a que monedas componen los precios y tratar de llevar todos sus valores a única moneda y siendo que el dólar es la moneda habitual lo mejor sería expresarlo en dólares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Las cantidad por currency es las siguiente:\\n{properati.currency.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la muestra se detecta que PEN y UYU parecieran un error de carga y que dado su cantidad son candidatos a ser eleminados.\n",
    "\n",
    "Con los precios en ARS se hará una tranformación a dólares en base a la tasa de cambio derivada de la columna price_aprox_local_currency/price para los price con currency en USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos el describe de la tasa de cambio\n",
    "(properati['price_aprox_local_currency'].loc[properati['currency']=='USD']/properati['price'].loc[properati['currency']=='USD']).describe().apply(lambda x: round(x,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que la tasa 17.64 es única para todas las propiedades expresadas en dólares y por ende es el valor que tomaremos para tranformar la columna price a price_usd que usaremos en adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Me quedo por la media porque en todo caso es lo mismo que el ~= al máximo y en caso de un año sería mejor\n",
    "tasaARSUSD = (properati['price_aprox_local_currency'].loc[properati['currency']=='USD']/properati['price'].loc[properati['currency']=='USD']).mean()\n",
    "tasaARSUSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero la columna priceUSD\n",
    "properati['priceUSD'] = properati.apply(lambda x: round(x['price'] / tasaARSUSD,0) if x['currency'] == 'ARS' else x['price'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati[['price','price_aprox_local_currency','currency','priceUSD']].loc[properati['currency']=='ARS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Buscar con Regex el precio en la descripción\n",
    "Una vez realizada la unificación de la monedad se procedera a buscar dentro de las descripciones cualquier monto expresado en ARS o USD y también se lo va a unificar a dólares.\n",
    "\n",
    "Para eso el primer paso será armar la expresión regular por la cual encontrar dichos valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armo el pattern para encontrar los montos en las descripciones\n",
    "pattern = '(?P<currency>U\\$D|ARS\\s*\\$|USD\\s*\\$?)\\s*(?P<price>\\d{1,3}(?:\\.?\\d{3})*(?:\\,\\d+)*)'\n",
    "pattern_regex = re.compile(pattern)\n",
    "\n",
    "# Lo fuerzo a string porque me dio un error pero no creo que sea necesario\n",
    "descripcion = properati.description.astype('str')\n",
    "\n",
    "# Busco todas las coincidencias posibles\n",
    "resultado = descripcion.apply(lambda x: pattern_regex.findall(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamo a la función que arme para descomponer la lista de listas que devuelve el findall\n",
    "regexPriceUSD = sacarValores(resultado,tasaARSUSD)\n",
    "\n",
    "#Lo convierto en serie para unir y pisar properati.priceUSD\n",
    "impRegex = pd.Series(regexPriceUSD)\n",
    "\n",
    "# Uno lo que generó el Regex al DF de properati\n",
    "properati['impRegex'] = impRegex\n",
    "\n",
    "# Esta es la cantidad de nulos que deberían quedar\n",
    "print(properati.loc[(properati['priceUSD'].isnull())]['ID'].count()-properati.loc[(properati['priceUSD'].isnull())&(~properati['impRegex'].isnull())]['ID'].count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En las filas vacias de properati['priceUSD'] le imputo el valor que devolvió el Regex\n",
    "properati['priceUSD'] = properati[['priceUSD','impRegex']].apply(lambda x: x['impRegex'] if pd.isnull(x['priceUSD']) else x['priceUSD'], axis=1)\n",
    "\n",
    "properati.priceUSD.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Análisis de Nulls en los campos [priceUSD]\n",
    "Siendo que restan imputar 19956 valores a pricesUSD se proceda a trata de completar los registros pendientes con otro metodo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"La cantidad de nulos en las columnas [priceUSD] es de: {properati['priceUSD'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#almaceno la serie de true/false de los price nulls\n",
    "filtroPriceUSDNull = properati.priceUSD.isnull()\n",
    "filtroPriceUSDNull.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Almaceno en un dataframe solo los valores nulos para saber como afrontarlo\n",
    "soloNulos = properati[filtroPriceUSDNull]\n",
    "soloNulos.shape[0] == properati['priceUSD'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por el volumen de nulls sería importante simplificar el problema para darle prioridad a los casos y poder simplificar la imputación. Para eso, lo primero que podemos considerar es como se comportan ante la ubicación por lo que a continuación trataremos de determinar si se comportan según el principio de Pareto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulosPorUbicacion = soloNulos.groupby(['place_with_parent_names'])['place_with_parent_names'].count()\n",
    "paretoNulos = nulosPorUbicacion.to_frame()\n",
    "paretoNulos.columns = ['cantidad']\n",
    "paretoNulos = paretoNulos.sort_values(by= 'cantidad', ascending=False)\n",
    "paretoNulos['pareto'] = paretoNulos['cantidad'].cumsum()/paretoNulos['cantidad'].sum()*100\n",
    "\n",
    "print(f\"El 80% de los casos se acumula en el {round((paretoNulos[paretoNulos['pareto'] <=80].shape[0]/paretoNulos.shape[0])*100,ndigits=2)}% de las ubicaciones\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo que podemos ver está mucho más concetrado que la proporción de Pareto por lo que si nos concentramos en estos lugares podriamos lograr completar más nulls con el menor análisis posible y comprender si es replicable en los demás."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo una lista de las ubicaciones que tienen más nulos\n",
    "maskUbicacionConMasNulos = paretoNulos[paretoNulos['pareto'] <=80].index\n",
    "\n",
    "# Creo un dataframe con los registros de las ubicaciones más nulas que tienen precio\n",
    "conPrecioMasUbicacionesNulas = properati[(~filtroPriceUSDNull) & (properati.place_with_parent_names.isin(maskUbicacionConMasNulos))]\n",
    "\n",
    "# Los agrupo para despues unirlo al análisis de pareto\n",
    "paraJoinConPareto = conPrecioMasUbicacionesNulas.groupby(['place_with_parent_names'])['place_with_parent_names'].count()\n",
    "dfParaJoinConPareto = paraJoinConPareto.to_frame()\n",
    "dfParaJoinConPareto.columns = ['cantidadConPrecio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno el agrupado de los que más nulos tienen con la cantidad de precios que tienen esas mismas ubicaciones en el dataset original.\n",
    "\n",
    "La idea es imputar por la media los nulos de cada ubicacion, pero para mitigar el efecto adverso de reducir la variabilidad solo lo voy a aplicar sobre las regiones donde por cada nulo tenga más de 10 precios reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armo el DF con las columnas de cantidad de nulos por región y la cantidad de precios de esa región\n",
    "ratiosNulosSobreConPrecio = paretoNulos.join(dfParaJoinConPareto)\n",
    "\n",
    "# Creo el ratios de precio por nulo\n",
    "ratiosNulosSobreConPrecio['ratioSinConPrice'] = (ratiosNulosSobreConPrecio['cantidadConPrecio']/ratiosNulosSobreConPrecio['cantidad'])\n",
    "\n",
    "# Muestro el impacto de la correción\n",
    "print(f\"Se podrían corregir por este metodo {ratiosNulosSobreConPrecio[ratiosNulosSobreConPrecio['ratioSinConPrice'] > 10].cantidad.sum()} nulls\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detalle de ubicaciones a corregir por imputación de media\n",
    "ratiosNulosSobreConPrecio[ratiosNulosSobreConPrecio['ratioSinConPrice'] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este sería el grupo donde el ratio es mayor que el limite que fijamos de tolerancia\n",
    "ratiosNulosSobreConPrecio[ratiosNulosSobreConPrecio['ratioSinConPrice'] <= 10].sort_values(by='ratioSinConPrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De los casos donde por cada nulo tenemos menos de 10 precios originales podemos ver que hay regiones donde practicamente no hay precios originales por lo que tendremos que buscar otra manera de imputarlas o estimarlas.\n",
    "\n",
    "Llegado el caso que no posamos hacerlo no nos va a quedar más que dar de baja esos nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo una lista con los lugares que voy a analizar\n",
    "listaLugares = ratiosNulosSobreConPrecio[ratiosNulosSobreConPrecio['ratioSinConPrice'] > 10].index\n",
    "\n",
    "# Armo un diccionario de control\n",
    "dccQs = getQs(conPrecioMasUbicacionesNulas,listaLugares,'priceUSD')\n",
    "pd.DataFrame(dccQs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por cada lugar grafico su distribución\n",
    "for lugar in listaLugares:\n",
    "    # Grupo de cada lugar\n",
    "    # Sería bueno meter esto en la función de getQs\n",
    "    subset = conPrecioMasUbicacionesNulas[conPrecioMasUbicacionesNulas['place_with_parent_names'] == lugar].priceUSD\n",
    "    # Armo el distplot por cada subset que arme en base a los lugares\n",
    "    #sns.set(rc={\"figure.figsize\": (16, 8)})\n",
    "    sns.set_style(\"dark\")\n",
    "    sns.distplot(subset, hist = False, kde = True,\n",
    "                 kde_kws = {'linewidth': 3},\n",
    "                 label = lugar)\n",
    "    # Formateo\n",
    "    plt.legend(prop={'size': 9}, title = 'Lugares')\n",
    "    plt.title('Distribución de los precios')\n",
    "    plt.xlabel('Precio')\n",
    "    plt.figure()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Primeras concluciones de las distribuciones para la imputación por media\n",
    "\n",
    "De los valores estadisticos descriptivos y de la gráfica de las distribuciones (aproximadas) podes detectar que tenemos una gran cantidad de ouliers por defecto y por exceso.\n",
    "\n",
    "Hablamos por defecto ya que los valores minimos por cada zona están muy por debajo de lo que podría ser un valor razonable. Por ejemplo si tenemos en cuanto el código de edificación de la C.A.B.A [(Link al código)](http://www2.cedom.gob.ar/es/legislacion/normas/codigos/edifica/index3a.html) la vivienda mínima es de 20.5 M2 de superficie cubierta, siendo este la menor superficie aprobable en todo el país, y si tenemos en cuenta que el precio promedio del M2 en toda la C.A.B.A [(Link a data set de USDxM2 de 2AMB y 3AMB)](https://data.buenosaires.gob.ar/dataset/mercado-inmobiliario/archivo/c6d2a64a-f60b-4b6e-9829-919139a0c1d1) es de ~ USD 2.535 por M2 es razonable esperar que los precios minimos de cualquier propiedad ronden por lo menos entorno de los ~ USD 51.000 un valor muy seprior a los minimos de muchas regiones en cuestión.\n",
    "\n",
    "Los valores por exceso son más complejos ya que pueden ser lugares con muchos M2 o en zonas donde el M2 es muy alto o ambas. Pero queda claro que esto convierte estas propiedades en excepciones (en las gráficas se vé que las tails de la distribución no suman mucha frequencia) y por lo tanto tampoco sería convenientes considerarlas para una imputación por media.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero un diccionario para la correción\n",
    "diccCorrecion = {}\n",
    "for lugar in listaLugares:\n",
    "    algo = properati.loc[properati.place_with_parent_names == lugar]\n",
    "    aca = algo.loc[(properati.priceUSD >= dccQs[lugar]['linf']) & (properati.priceUSD >= dccQs[lugar]['lsup'])]\n",
    "    diccCorrecion[lugar] = round(aca.priceUSD.mean(),2)\n",
    "    \n",
    "diccCorrecion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputo con un apply los valores de la media que tenía en el diccionario\n",
    "properati['priceUSDImp'] = properati.apply(lambda x: diccCorrecion[x['place_with_parent_names']] if (x['place_with_parent_names'] in diccCorrecion.keys()) & (pd.isnull(x['priceUSD'])) else x['priceUSD'] ,axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati.loc[(properati['priceUSD'].isnull())&(~properati['priceUSDImp'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Armo el diccionario para comprar controlar los cambios en la distribución\n",
    "dccPosterior = getQs(properati,listaLugares,'priceUSDImp')\n",
    "dccControl = getQs(properati,listaLugares,'priceUSD')\n",
    "pd.DataFrame(dccPosterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dccControl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dccControl)-pd.DataFrame(dccPosterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properati.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"La nueva cnatidad de nulos en la columnas priceUSD debería ser {properati['priceUSDImp'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En las filas vacias de properati['priceUSD'] le imputo el valor que devolvió la media de las zonas\n",
    "properati['priceUSD'] = properati[['priceUSD','priceUSDImp']].apply(lambda x: x['priceUSDImp'] if pd.isnull(x['priceUSD']) else x['priceUSD'], axis=1)\n",
    "\n",
    "properati[['price','priceUSD']].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Análisis de mínimos\n",
    "En este punto evaluaremos el impacto de realizar lo que se planteo en el punto 3.1. respecto de los valores mínimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armo un DF solo con los precios en dóalres y su ubicación\n",
    "analisisMinimos = properati[['priceUSD','place_with_parent_names']]\n",
    "\n",
    "# Pivoteo el DF para que cada columna sea una ubicación\n",
    "pivotAnalisisMinimos = analisisMinimos.pivot(columns='place_with_parent_names',values='priceUSD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compruebo que se mantenga la proporción\n",
    "print(pivotAnalisisMinimos.shape)\n",
    "\n",
    "# Obtengo el describe de cada ubicación\n",
    "describe = pivotAnalisisMinimos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtengo la cantidad de filas candidatas\n",
    "print(pivotAnalisisMinimos[pivotAnalisisMinimos <= 51000].count().sum())\n",
    "\n",
    "#\n",
    "print(describe.loc['std'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Concluciones de la limpieza\n",
    "\n",
    "Se logró reducir el nro de nulos de 20.410 (\\~16% del total) a 18.806 (\\~15% del total), pero nos permitio detectar muchas puntos de mejora y carasteristicas del dataset a la largo del análisis.\n",
    "\n",
    "**1.** Las distribuciones dentro de las zonas parecieran ser muy dispersas.\n",
    "   * Sería conveniente aperturar por tipo de propiedad para mejorar la imputación por media reduciendo la variabilidad por zona.\n",
    "   * Sería prudente descartar los valores que no se aproximen a un precio mínimo razonable ya que valores de USD 1 solo perjudican la estimación.\n",
    "\n",
    "**2.** Si bien los barrios difieren mucho entre si todas las distribuciones presentan un fuerte sesgo hacia a la izquierda.\n",
    "   * Esto nos habla que hay una regla en común los precios más bajos son los que acumulan mayores cantidades.\n",
    "   * Se podría evaluar hacer un regresor del precio en base a los m2 y barrio (minimamente) para reemplazar la imputación por media y por Regex.\n",
    "   \n",
    "**3.** Si tomamos como valor mínimo USD 51.000 se perderían ~4.000 registros.\n",
    "   * Esto no sería de gran impacto sobre el total (\\~4% del total) y ayudaría a disminuir el inmenso std que se observa por barrio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
